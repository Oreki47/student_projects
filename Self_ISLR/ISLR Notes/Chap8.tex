\section{Tree-Based Methods}
\subsection{Notes}
Essentially for a regression tree (classification is similar), it contains two steps
\begin{enumerate}
	\setlength\itemsep{0em}
	\item divide the predictor space into $J$ distince and non-overlapping regions.
	\item For every observation taht falls into region $R_j$, we make the same prediction, which is simply the mean of the response values for the training observations in $R_j$.
\end{enumerate}

The problem then falls into how do we perform such partition. Turns out that almost all tree methods use a greedy approach (and therefore can be seen as a heuristic algorithm). Here we go through \xt{recursive binary splitting}. The idea is simple, for each round, we iterate through all predictors and for each predictor we find out the best split point, and then choose the predictor and cutpoint such that the resulting tree has the lowest RSS (could be other metrics, for instance, info-gain, or entropy). We define the pair of half-planes $$ R_1(j,s)=\{X|X_j < s\}\ \ \text{and}\ \ R_2(j,s) = \{X|X_j \geq s \},$$ and we seek the value of $j$ and $s$ that minimize the equation $$ \sum_{i:x_i \in R_1(j, s)} (y_i - \hat{y}_{R_1})^2 + \sum_{i:x_i \in R_2(j, s)} (y_i - \hat{y}_{R_2})^2,$$ where $\hat{y}_{R_1}$ is the mean response for the training observations in $R_1(j,s)$ and same goes to the other half.
% ----------------------------------------------------------------------
\subsubsection{Pruning}
Again there are many ways to do this. 
% ----------------------------------------------------------------------
\subsubsection{Classification tree}
In practice, classification error is not sufficiently sensitive for tree-growing, and in practice two other measures are preferable.
\begin{enumerate}
	\setlength{\itemsep}{0em}
	\item $G = \sum_{k=1}^{K}\hat{p}_{mk}(1- \hat{p}_{mk})$, which is the \xt{Gini index} -- which can be seen as a measure of total variance across the $K$ classes. It is obvious to see that the Gini index takes on a small value if all the $\hat{p}_{mk}$ are close to 0 or 1. For this reason the Gini index is referred to as a measure of node \xt{purity} -- a small value indicates that a node contains predominately observations from a single class.
	\item $D = -\sum_{k=1}^{K}\hat{p}_{mk}\log\hat{p}_{mk}$, which is the \xt{cross-entropy} -- the cross entropy will take on small value if the $m$th node is pure. 
\end{enumerate}
Simply put, these two matrices are quite similar. 
% ----------------------------------------------------------------------
\subsubsection{Bagging, Random Forests, Boosting}
The decision tree method suffers from high variance (since it has too strong of learning power). Here we go through three methods that help reduce the variance. It provides a convenient way to determine predictor importance.
\begin{enumerate}
	\setlength{\itemsep}{0em}
	\item Bagging -- Recall that given a set of $n$ independent observations, each with variance $\sigma^2$, the variance of the mean is given by $\sigma^2/n$. In other words, averaging a set of observations reduces variance. Hence, a natural way to reduce the variance is  to take many training sets from the population, build a separate prediction model using each training set, and average the resulting predictions. Of course this is not practical since we do not have multiple training sets. Instead, we could bootstrap, by taking repeated samples from the (single) training data set.
	\item Random Forests -- Apply a small tweak that de-correlates the trees in bagging. When building a tree, each time a random sample of $m$ predictors is chosen as asplit candidates from the full set of $p$ predictors. Typically $m \approx \sqrt{p}$.
	\item Boosting -- Originally, boosting means we dynamically update the sample weight. For sample classified in the wrong class, we increase its weight. What gradient boost does is, to decrease the residual (error), we could build a model that follow the direction of the gradient. I assume that boosting is a very general idea, i.e., to learn from previous mistakes. But there are many ways to implement such idea (general boosting, gradient boost, and what xgboost does).
\end{enumerate}